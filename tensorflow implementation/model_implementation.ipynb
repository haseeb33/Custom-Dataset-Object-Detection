{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "9yMCnWAw03vd",
    "outputId": "673dacc4-d148-4376-ee44-a85d0bb38a4b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Used Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "Ncs93qFt1XOy",
    "outputId": "67b1c899-ca44-4c52-e7af-f29c4d6beb69"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import json\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "# Have to use tensorflow version 1... because library has some issues with tf 2..\n",
    "%tensorflow_version 1.x\n",
    "# Have to use keras < 2.1.. because lib has problems with new keras\n",
    "!pip install keras==2.1.0\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lg8E6Cgy1f51"
   },
   "outputs": [],
   "source": [
    "class ClassConfig(Config):\n",
    "    NAME = \"classes\"\n",
    "\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    NUM_CLASSES = 1 + 1  # background + box\n",
    "\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)  # anchor side in pixels\n",
    "\n",
    "    # High training ROIs per image because the images are big and have\n",
    "    # many objects.\n",
    "    TRAIN_ROIS_PER_IMAGE = 512\n",
    "\n",
    "    # Small number of epoch with high steps per epoch becasue models.h5 file\n",
    "    # is saved after each epoch so to save memory\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "    # Skip detections with < 60% confidence\n",
    "    #DETECTION_MIN_CONFIDENCE = 0.6 May need latter\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "\n",
    "config = ClassConfig()\n",
    "#config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHm6g4fj9EJw"
   },
   "outputs": [],
   "source": [
    "class ClassDataset(utils.Dataset):\n",
    "  \n",
    "    def load_data(self, dataset_dir, json_file):\n",
    "        \n",
    "        # Add classes\n",
    "        self.add_class(\"classes\", 1, \"class\")\n",
    "        classes_dict = {\"class\":1} # add more in neccessary \n",
    "        # Load annotations\n",
    "        annotations = json.load(open(os.path.join(dataset_dir, json_file)))\n",
    "        annotations = list(annotations.values())  # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "        \n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            # Get the x, y coordinaets of points of the polygons that make up\n",
    "            # the outline of each object instance. There are stored in the\n",
    "            # shape_attributes \n",
    "            polygons = [r['shape_attributes'] for r in a['regions'].values()]\n",
    "            objects = [s['region_attributes'] for s in a['regions'].values()]\n",
    "\n",
    "            # load_mask() needs the image size to convert polygons to masks.\n",
    "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "            num_ids = [classes_dict[n['label']] for n in objects] \n",
    "            # the image. This is only managable since the dataset is tiny.\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"classes\",\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons,num_ids=num_ids)\n",
    "                \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\" Load instance masks for the given image.\n",
    "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
    "        Args:\n",
    "            image_id: The id of the image to load masks for\n",
    "        Returns:\n",
    "            masks: A bool array of shape [height, width, instance count] with\n",
    "                one mask per instance.\n",
    "            class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] != \"classes\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        num_ids = info['num_ids']\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids\n",
    "    \n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"classes\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tM7XKL-B-EQG"
   },
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "dataset_train = ClassDataset()\n",
    "dataset_train.load_data(\"Main/train\", \"train_label.json\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ClassDataset()\n",
    "dataset_val.load_data(\"Main/val\", \"val_label.json\")\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAmNQf-E5BXL"
   },
   "outputs": [],
   "source": [
    "# Training code block 1\n",
    "#Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETXmepbFqRQ_"
   },
   "outputs": [],
   "source": [
    "# Training code block 2\n",
    "# Training - Stage 1\n",
    "# Finetune heads only of Resnet \n",
    "print(\"Fine tune Resnet stage 4 and up\")\n",
    "model.train(dataset_train,dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=5,\n",
    "            layers='heads')\n",
    "\n",
    "# Training - Stage 2\n",
    "# Finetune layers from ResNet stage 4 and up \n",
    "#(this number can be changed and performance should be monitored)\n",
    "print(\"Fine tune Resnet stage 4 and up\")\n",
    "model.train(dataset_train,dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=10,\n",
    "            layers='4+')\n",
    "\n",
    "# Training - Stage 3\n",
    "# Fine tune all layers \n",
    "print(\"Fine tune all layers\")\n",
    "model.train(dataset_train,dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE/10,\n",
    "            epochs=15,\n",
    "            layers='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Lvi2N0V8Pq3"
   },
   "outputs": [],
   "source": [
    "# For model testing if needed update the configs\n",
    "class InferenceConfig(ClassConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.8\n",
    "\n",
    "inference_config = InferenceConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "xbHZwcmU8lVR",
    "outputId": "250ec232-7d7c-40f4-9968-e8042049c962"
   },
   "outputs": [],
   "source": [
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = 'Mask_RCNN-master/logs/classes20200713T0016/mask_rcnn_classes_0014.h5'\n",
    "#model_path = model.find_last()\n",
    "\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "DQEF32rUf3iN",
    "outputId": "085bd545-9242-46d9-d379-0db08cb7e392"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#For now testing on validation dataset\n",
    "real_test_dir = 'Main/test'\n",
    "image_paths = []\n",
    "for filename in os.listdir(real_test_dir):\n",
    "    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n",
    "        image_paths.append(os.path.join(real_test_dir, filename))\n",
    "\n",
    "predictions = []\n",
    "for image_path in image_paths:\n",
    "    img1 = Image.open(image_path).convert(\"RGB\") #png has 4 channels so to remove that extra channel and do processing\n",
    "    img = skimage.io.imread(image_path)\n",
    "    img_arr = np.array(img1)\n",
    "    results = model.detect([img_arr], verbose=1)\n",
    "    r = results[0]\n",
    "    predictions.append(r)\n",
    "    print(\"Current image is \", str(image_path))\n",
    "    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n",
    "                                dataset_val.class_names, r['scores'], figsize=(5,5), show_bbox=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "AIDzvQHzaTFu",
    "outputId": "e6b2fb7d-34db-44a9-e6e0-83426b64168e"
   },
   "outputs": [],
   "source": [
    "visualize.display_top_masks(img, r['masks'], r['class_ids'], dataset_val.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "Goe36g47aR0k",
    "outputId": "eae944ce-a98b-4451-849c-e4638d2165c6"
   },
   "outputs": [],
   "source": [
    "display_images([img]+[r['masks'][:,:,i] for i in range(r['masks'].shape[-1])], cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "Udv9-UbKpBIh",
    "outputId": "e1b5732c-6c9b-4c1b-e915-5c20ca0ca5ca"
   },
   "outputs": [],
   "source": [
    "#Cropping w.r.t boundary boxes\n",
    "def get_segment_crop(img,tol=0, mask=None):\n",
    "    if mask is None:\n",
    "        mask = img > tol\n",
    "    return img[np.ix_(mask.any(1), mask.any(0))]\n",
    "\n",
    "display_images([img]+[get_segment_crop(img, mask=r['masks'][:,:,i]) for i in range(r['masks'].shape[-1])], cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "JfZZj1tvdvGA",
    "outputId": "c8266b25-df22-4c26-e2c0-a897b09b257c"
   },
   "outputs": [],
   "source": [
    "# Cropping w.r.t mask region and boundry box\n",
    "def to_rgb(im, datatype=np.uint8):\n",
    "    w, h = im.shape\n",
    "    ret = np.empty((w, h, 3), dtype=datatype)\n",
    "    ret[:, :, 2] =  ret[:, :, 1] =  ret[:, :, 0] =  im\n",
    "    return ret\n",
    "\n",
    "display_images([img]+[get_segment_crop(img*to_rgb(r['masks'][:,:,i]), mask=r['masks'][:,:,i]) for i in range(r['masks'].shape[-1])], cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "dXHyEJ_X4sv_",
    "outputId": "b1e596f6-0294-49cd-99d7-f4dc09043816"
   },
   "outputs": [],
   "source": [
    "cols = 8\n",
    "rows = r['masks'].shape[-1] // cols + 1\n",
    "plt.figure(figsize=(14, 14 * rows // cols))\n",
    "\n",
    "for i in range(r['masks'].shape[-1]):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    a = get_segment_crop(img, mask=r['masks'][:,:,i])\n",
    "    rgba = cv2.cvtColor(a, cv2.COLOR_RGB2RGBA)\n",
    "    rgba[:, :, 3] = get_segment_crop(r['masks'][:,:,i]*255, mask=r['masks'][:,:,i])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rgba)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAiufkPU8m6_"
   },
   "outputs": [],
   "source": [
    "# Finding the topmost, bottommost, leftmost, rightmost point\n",
    "# But we need top left, top right, bottom left and bottom right, before skewing the image\n",
    "\n",
    "mask = r['masks'][:,:,0]\n",
    "idx0 = np.nonzero(mask.ravel())[0]\n",
    "idx1 = np.nonzero(mask.ravel())[0]\n",
    "idxs = [idx0.min(), idx0.max(), idx1.min(), idx1.max()]\n",
    "out = np.column_stack(np.unravel_index(idxs,mask.shape))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vgCAznER-94m",
    "outputId": "5af2fd5d-03e6-487f-c801-ce0e6dc63d9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1452, 195), (1716, 562)]"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the topmost, bottommost, leftmost, rightmost point\n",
    "# But we need top left, top right, bottom left and bottom right, before skewing the image\n",
    "# Similar approach as mentioned above\n",
    "\n",
    "array = np.float32(mask)\n",
    "H,W = array.shape\n",
    "left_edges = np.where(array.any(axis=1),array.argmax(axis=1),W+1)\n",
    "flip_lr = cv2.flip(array,1) #1 horz vert 0\n",
    "right_edges = W-np.where(flip_lr.any(axis=1),flip_lr.argmax(axis=1),W+1)\n",
    "top_edges = np.where(array.any(axis=0),array.argmax(axis=0),H+1)\n",
    "flip_ud = cv2.flip(array,0) #1 horz vert 0\n",
    "bottom_edges = H - np.where(flip_ud.any(axis=0),flip_ud.argmax(axis=0),H+1)\n",
    "leftmost = left_edges.min()\n",
    "rightmost = right_edges.max()\n",
    "topmost = top_edges.min()\n",
    "bottommost = bottom_edges.max()\n",
    "bb = [(leftmost, topmost), (rightmost, bottommost)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzaTCglNc3zr"
   },
   "outputs": [],
   "source": [
    "# Extract corners first using cv2 which are in terms of cordinates (x,y)\n",
    "# Problem is cv2 finds many points as corners but we need to extract only 4 corners(top left, topright, ... ....)\n",
    "\n",
    "corners = cv2.goodFeaturesToTrack(array, 100, 0.01, 10) \n",
    "corners = np.int0(corners) \n",
    "\n",
    "for i in corners: \n",
    "    x, y = i.ravel()   \n",
    "    #cv2.circle(img, (x, y), 3, 255, -1)\n",
    "\n",
    "plt.imshow(img), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLhG00g0PTX8"
   },
   "outputs": [],
   "source": [
    "# This method is not working\n",
    "contours, hier = cv2.findContours(np.int16(array), cv2.RETR_FLOODFILL, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rVGAiAItRJ3"
   },
   "outputs": [],
   "source": [
    "#Once we can find the cordinates of top right, top left, bottom right, bottom left\n",
    "# we can easily skew our images to desired rectangular\n",
    "\n",
    "all_plots = [img]\n",
    "for i in range(r['masks'].shape[-1]):\n",
    "  #rows,cols,ch = img.shape\n",
    "  pts1 = np.where(r['masks'][:,:,i])\n",
    "  pts2 = np.float32([[0,0],[150,0],[0,150],[150,150]])\n",
    "  M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "  dst = cv2.warpPerspective(img,M,(150,150))\n",
    "  all_plots += [dst]\n",
    "\n",
    "display_images(all_plots, cols=8)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Box_segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
